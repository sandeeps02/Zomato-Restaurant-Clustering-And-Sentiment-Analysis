{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeeps02/Zomato-Restaurant-Clustering-And-Sentiment-Analysis/blob/main/Zomato_Restaurant_Clustering_And_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Zomato Restaurant Clustering and Sentiment Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Zomato Restaurant Clustering and Sentiment Analysis project focuses on analyzing Zomato's restaurant data from various cities in India. The goal is to cluster restaurants into different segments based on their attributes and analyze customer sentiments through reviews. By using visualizations, we aim to provide valuable insights for customers to find the best restaurants in their area and offer the company recommendations for improvements. The analysis will include cost vs. benefit assessments using cuisine and pricing data, with the ultimate aim of helping both customers and the company make informed decisions in the restaurant industry."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/sandeeps02/Zomato-Restaurant-Clustering-And-Sentiment-Analysis"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis: Perform sentiment analysis on user reviews to understand\n",
        "customer sentiments towards various restaurants. The analysis should classify reviews as positive, negative, or neutral, providing insights into customer satisfaction and identifying areas for improvement.\n",
        "\n",
        "Restaurant Clustering: Apply clustering algorithms to group restaurants into distinct segments based on their attributes. The clustering should reveal patterns and similarities among restaurants, allowing for better recommendations to customers and strategic decisions for the company."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Display all the columns in the dataframe\n",
        "pd.pandas.set_option(\"display.max_columns\", None)"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qy02PwG-R_lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset1\n",
        "review= pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zomato dataset/Zomato Restaurant reviews.csv\")\n",
        "# Load Dataset2\n",
        "restrodata=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zomato dataset/Zomato Restaurant names and Metadata.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset1 & 2 First Look\n",
        "review.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.head()"
      ],
      "metadata": {
        "id": "L1p3uXtsSVDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "review.shape,restrodata.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "review.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.info()"
      ],
      "metadata": {
        "id": "XAZ8qj7VShDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "review.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review.loc[review.duplicated()]"
      ],
      "metadata": {
        "id": "JyTxDr45SpP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.duplicated().sum()"
      ],
      "metadata": {
        "id": "s7myBSP7Suo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "review.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.isnull().sum()"
      ],
      "metadata": {
        "id": "Z_SnHiEbSz0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "review_missing_percent=review.isnull().sum()/ len(review)*100\n",
        "restrodata_null_percent= restrodata.isnull().sum()/ len(restrodata)* 100"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x=review_missing_percent.index, y=review_missing_percent)"
      ],
      "metadata": {
        "id": "-gn-WP3mS5TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x=restrodata_null_percent.index, y=restrodata_null_percent)"
      ],
      "metadata": {
        "id": "CZOL9q6nS7xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "review.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.columns"
      ],
      "metadata": {
        "id": "Y1xUYlArTB3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "review.describe(include=\"all\").T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.describe().T"
      ],
      "metadata": {
        "id": "2tA5kcXHTF0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "# For Restorant data\n",
        "for col in restrodata:\n",
        "    if restrodata[col].dtype==object:\n",
        "        print(restrodata[col].value_counts())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in review:\n",
        "    if review[col].dtype==object:\n",
        "        print(review[col].value_counts())"
      ],
      "metadata": {
        "id": "YSyjNVpaTPBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Restorant data\n",
        "\n",
        "restrodata[\"Cost\"]=restrodata[\"Cost\"].str.replace(\",\" , \"\").astype(int)"
      ],
      "metadata": {
        "id": "bDXYJcq5TVee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.Cost.unique()"
      ],
      "metadata": {
        "id": "Z80yg2hBTWLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Review\n",
        "## For Metadat, Sperating columns and followers\n",
        "review[\"Review_count\"] = review[\"Metadata\"].str.split(\",\").str[0]\n",
        "review[\"Review_count\"] = review[\"Review_count\"].str.split(\" \").str[0]\n",
        "review[\"Followers\"] = review[\"Metadata\"].str.split(\",\").str[1]\n",
        "review[\"Followers\"] = review[\"Followers\"].str.split(\" \").str[1]"
      ],
      "metadata": {
        "id": "oKSiwJrsTX5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Rating there is on \"like value\", we gonna replace that\n",
        "review['Rating'] = review['Rating'].replace('Like', review[review['Rating'] != 'Like']['Rating'].astype(float).median())\n",
        "review['Rating'] = pd.to_numeric(review['Rating'], errors='coerce')\n",
        "review[\"Review_count\"]=pd.to_numeric(review[\"Review_count\"],errors=\"coerce\")"
      ],
      "metadata": {
        "id": "83m4T3idTggo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting date/time from time column\n",
        "# fOR RESTAURANT\n",
        "review['Time'] = pd.to_datetime(review['Time'])\n",
        "review['Review_Year'] = review['Time'].dt.year\n",
        "review['Review_Month'] = review['Time'].dt.month\n",
        "review['Review_Hour'] = review['Time'].dt.hour\n",
        "review.head(2)"
      ],
      "metadata": {
        "id": "g2E5tlE3Tp8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.head(2)"
      ],
      "metadata": {
        "id": "5PXy92chTtnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Restaurant Data"
      ],
      "metadata": {
        "id": "wYg8JseBT2G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "top10_restro=restrodata.groupby(\"Name\")[\"Cost\"].max().nlargest(10)\n",
        "sns.barplot(y=top10_restro.index, x=top10_restro.values)\n",
        "plt.ylabel('Restaurant Name')\n",
        "plt.xlabel('Cost (Per Person)')\n",
        "plt.title('Top 10 Restaurants based on Cost')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm using barplots to discover the priciest restaurants."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing the graph, it becomes evident that Hyatt Hyderabad Gachibowli, Sheraton Hyderabad Hotel, and 10 Downing Street are the top three costliest restaurants based on their per-person cost."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "top5_economy_restro = restrodata.groupby(\"Name\")[\"Cost\"].min().nsmallest(5)\n",
        "sns.barplot(x=top5_economy_restro.values, y=top5_economy_restro.index)\n",
        "plt.xlabel('Cost (Per Person)')\n",
        "plt.ylabel('Restaurant Name')\n",
        "plt.title('Top 5 Economy Restaurant')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the bar plot visualization, I have investigated and identified the most budget-friendly restaurants, providing valuable insights into the economical culinary options available for diners."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the data represented in the graph, it is evident that Amul, Mohammedia Shawarma, and Asian Meal Box are the most affordable restaurants, offering budget-friendly dining options to customers."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Split the cuisines and store them in a list\n",
        "cuisine_list = restrodata['Cuisines'].str.split(', ').explode()\n",
        "\n",
        "cuisine_data = cuisine_list.value_counts().reset_index()\n",
        "cuisine_data.columns = ['Cuisine', 'Number of Restaurants']\n",
        "\n",
        "# Select the top 10 cuisines based on occurrence\n",
        "top10cuisine = cuisine_data.nlargest(10, 'Number of Restaurants')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Number of Restaurants', y='Cuisine', data=top10cuisine)\n",
        "plt.xlabel('Number of Restaurants')\n",
        "plt.ylabel('Cuisine')\n",
        "plt.title('Top 10 Cuisines by Occurrence')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By utilizing the barplot visualization technique, I am able to present a clear and insightful view of the most sought-after cuisines, highlighting the culinary preferences that are in high demand among consumers."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the graph, it is evident that North Indian, Chinese, and Continental cuisines are the most in-demand and widely available options in restaurants. These cuisines enjoy a significant presence and popularity among customers."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Storing all cuisines in the form of text\n",
        "text = \" \".join(name for name in cuisine_data.Cuisine)\n",
        "\n",
        "# Creating the word cloud with text as an argument in .generate() method\n",
        "word_cloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud for Cuisines\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the utilization of a word cloud, I was able to visually identify the most prevalent cuisines, showcasing the dominant culinary choices that are abundantly available across various restaurants."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word cloud visualization prominently displays North Indian, Chinese, and Continental cuisines as the most prevalent and frequently offered options among various restaurants. Their larger appearance in the word cloud indicates their higher representation in the data."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Convert the 'Collections' column to string and remove NaN values\n",
        "restrodata['Collections'] = restrodata['Collections'].astype(str).replace('nan', '')\n",
        "\n",
        "# Storing all collections in the form of text\n",
        "text = \" \".join(name for name in restrodata.Collections)\n",
        "\n",
        "# Creating the word cloud with text as an argument in .generate() method\n",
        "word_cloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "4_RL0-MkU7mZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By employing the word cloud visualization, I have effectively captured and represented the most frequently used tags, providing a visually striking depiction of the prevalent themes and topics that are widely utilized in the dataset."
      ],
      "metadata": {
        "id": "6p_jcLQPU7ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the word cloud graph, it is apparent that \"Hyderabad,\" \"food hygiene,\" and \"rated restaurants\" are the most commonly used tags employed by the restaurants. Their larger appearance in the word cloud highlights their significant prevalence and relevance in the datase"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#For Restaurants Review"
      ],
      "metadata": {
        "id": "TiJ-1GS-VGW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "top10_rated = review.groupby(\"Restaurant\")[\"Rating\"].max().nlargest(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top10_rated.values, y=top10_rated.index, palette='viridis')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Restaurant')\n",
        "plt.title('Top 10 Restaurants based on Maximum Ratings')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By employing the barplot visualization, I have successfully identified and displayed the top-rated restaurants, offering a clear and concise view of the dining establishments that have received the highest accolades and positive feedback from customers."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Among them, 10 Downing Street, 13 Dhaba, and Barbeque Nation emerge as the most highly rated choices.Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Sort the restaurants based on their minimum ratings in ascending order\n",
        "sorted_restaurants = review.groupby(\"Restaurant\")[\"Rating\"].min().sort_values()\n",
        "\n",
        "# Select the top 5 restaurants with the lowest ratings\n",
        "top5_least_rated = sorted_restaurants.nsmallest(5)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top5_least_rated.values, y=top5_least_rated.index, palette='viridis')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Restaurant')\n",
        "plt.title('Top 5 Restaurants based on the Lowest Ratings')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the bar plot visualization, I have investigated the least rated restaurants."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the barplot analysis, it becomes apparent that 10 Downing Street, Prism Club Kitchen, and Pourhouse7 are among the least rated restaurants, indicating that these establishments have received comparatively lower customer ratings and feedback. This information highlights potential areas for improvement and further attention to enhance their overall dining experiences."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data=review, x='Rating', bins=10, kde=True)\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Histogram of Ratings')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "By utilizing the histogram visualization, I am able to identify the most frequently assigned rating scores, offering valuable insights into the preferred or common rating categories given by customers to various restaurants."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per the histogram analysis of the dataset, it is evident that the majority of customers have given a 5-star rating to the restaurants. This indicates that a significant number of diners have had highly satisfactory experiences, leading to the prevalence of 5-star ratings as the most common rating category."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize = (15, 5))\n",
        "sns.heatmap(review.corr(),ax = ax, annot=True, cmap = 'icefire', linewidths = 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review=review.drop_duplicates()"
      ],
      "metadata": {
        "id": "Pks88eAxWJH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming restaurant column name to restaurant\n",
        "restrodata=restrodata.rename(columns={\"Name\":\"Restaurant\"})"
      ],
      "metadata": {
        "id": "qCWhBxA4Xrwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging review and restrodata\n",
        "merge_data=restrodata.merge(review, on=\"Restaurant\")"
      ],
      "metadata": {
        "id": "OOgLH6Y2XsYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Null Values\n",
        "merge_data.isnull().sum()"
      ],
      "metadata": {
        "id": "uRXaX7FuYoUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Rows and column\n",
        "merge_data.shape"
      ],
      "metadata": {
        "id": "DPaUENABYpMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Null values in Timings\n",
        "merge_data.loc[merge_data[\"Timings\"].isnull()]"
      ],
      "metadata": {
        "id": "486dVxJlYv5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling Timing null value\n",
        "merge_data.Timings.fillna(merge_data.Timings.mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "G0QtMKDtYxvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Nan in Columns\n",
        "merge_data=merge_data.dropna(subset=[\"Review\", \"Review_count\"])"
      ],
      "metadata": {
        "id": "-RTHcH8CY0Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values in review and reviewer follower column\n",
        "merge_data= merge_data.fillna({\"Review\": \"No Review\"})"
      ],
      "metadata": {
        "id": "CLnoLvEAY2E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Null\n",
        "merge_data.isnull().sum()"
      ],
      "metadata": {
        "id": "hvuV5M-KY3jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Outliers\n",
        "merge_data.describe()"
      ],
      "metadata": {
        "id": "aQTJi_PCZBGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data['Followers'] = pd.to_numeric(merge_data['Followers'], errors='coerce')\n"
      ],
      "metadata": {
        "id": "m_GmiJ3eZz_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Defining a function for calcualting outliers-\n",
        "def calculate_outlier(df, column):\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] > upper) | (df[column] < lower)]\n",
        "    percent_outliers = round((outliers.shape[0] / df.shape[0]) * 100, 2)\n",
        "    return lower, upper, percent_outliers"
      ],
      "metadata": {
        "id": "GU_SkdEWZCRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_cost, upper_cost, percentage_cost_outliers=calculate_outlier(merge_data, \"Cost\")\n",
        "print(\"lower band\",(lower_cost))\n",
        "print(\"upper band\",(upper_cost))\n",
        "print(\"outlier percent\",(percentage_cost_outliers))"
      ],
      "metadata": {
        "id": "iLBlXN0nZLva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data.loc[merge_data[\"Cost\"]> upper_cost, \"Cost\" ]=2250"
      ],
      "metadata": {
        "id": "B-0tYBuQaWP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_count, upper_count, followers_percentage_outliers=calculate_outlier(merge_data, \"Followers\")\n",
        "print(\"lower band\",(lower_count))\n",
        "print(\"upper band\",(upper_count))\n",
        "print(\"outlier percent\",(followers_percentage_outliers))"
      ],
      "metadata": {
        "id": "XBHqxn_cacEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data.loc[merge_data[\"Followers\"]> upper_count, \"Followers\" ]=227"
      ],
      "metadata": {
        "id": "k48nAd5sacq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Textual Data Preprocessing\n"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###For Review"
      ],
      "metadata": {
        "id": "ggMWnm0ga_ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data.Review.head(8)"
      ],
      "metadata": {
        "id": "0gmFe2wubSaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "id": "CCIurzDrbe-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    # Using the contractions library to expand contractions in the text\n",
        "    expanded_text = contractions.fix(text)\n",
        "    return expanded_text"
      ],
      "metadata": {
        "id": "oJE39Bb9bS0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data[\"Review\"] = merge_data[\"Review\"].apply(expand_contractions)"
      ],
      "metadata": {
        "id": "0x9G6QhxbVhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "def to_lower(text):\n",
        "    lower_text=text.lower()\n",
        "    return lower_text"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data[\"Review\"] = merge_data[\"Review\"].apply(to_lower)"
      ],
      "metadata": {
        "id": "eKWB7bhwbp8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def remove_punc(text):\n",
        "    # Normalize text by removing accents and converting to NFC form\n",
        "    normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "\n",
        "    # Remove punctuation characters from the text, except for alphabets and numbers\n",
        "    punc_text = re.sub('[^a-zA-Z0-9]', ' ', normalized_text)\n",
        "\n",
        "    return punc_text"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data[\"Review\"] = merge_data[\"Review\"].apply(remove_punc)"
      ],
      "metadata": {
        "id": "GHLpa3ydbxSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_urls(text):\n",
        "    # Convert the input to a string if it's not already\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove URLs using regular expression\n",
        "    url_pattern = r'http\\S+|www\\S+'\n",
        "    no_urls_text = re.sub(url_pattern, '', text)\n",
        "    return no_urls_text\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data[\"Review\"] = merge_data[\"Review\"].apply(remove_urls)"
      ],
      "metadata": {
        "id": "MbS4qLMOb1Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Tokenize the text into individual words\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords from the list of words\n",
        "    stopwords_list = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stopwords_list]\n",
        "\n",
        "    # Join the remaining words back into a single string\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data[\"Review\"] = merge_data[\"Review\"].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "wO3cfISlb97X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "aw9tQvMkcZDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def word_token(text):\n",
        "    tokens=nltk.word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data[\"Review\"] = merge_data[\"Review\"].apply(word_token)"
      ],
      "metadata": {
        "id": "mbNeg52dcH29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data.Review.head(5)"
      ],
      "metadata": {
        "id": "Eviq0vZAcJ4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_words(text):\n",
        "    stemmer=PorterStemmer()\n",
        "    stemmed_words=[stemmer.stem(words) for words in text]\n",
        "    return stemmed_words"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data[\"Review\"] = merge_data[\"Review\"].apply(stem_words)"
      ],
      "metadata": {
        "id": "EbQaiXiXcoox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis"
      ],
      "metadata": {
        "id": "pQQpaeDQc_mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentiment lexicon\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "FL8WRXCFc_AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Vender SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment score for each review\n",
        "def get_sentiment_score(review):\n",
        "    review = ' '.join(review) # Convert list of words back to a sentence\n",
        "    return sia.polarity_scores(review)['compound']\n",
        "\n",
        "# Apply the sentiment analysis function to the 'Review' column\n",
        "merge_data['Vader_Sentiment'] = merge_data[\"Review\"].apply(get_sentiment_score)"
      ],
      "metadata": {
        "id": "a9LZarErdXmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "JbXKpJK5dggg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SentiWordNet\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "nltk.download('sentiwordnet')\n",
        "\n",
        "def get_sentiwordnet_sentiment(review):\n",
        "    sentiment_score = 0\n",
        "    for word in review:\n",
        "        synsets = list(swn.senti_synsets(word))\n",
        "        if synsets:\n",
        "            sentiment_score += synsets[0].pos_score() - synsets[0].neg_score()\n",
        "    return sentiment_score\n",
        "\n",
        "merge_data[\"SentiWordNet_Sentiment\"] = merge_data[\"Review\"].apply(get_sentiwordnet_sentiment)"
      ],
      "metadata": {
        "id": "hdsgmegQdYAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install afinn"
      ],
      "metadata": {
        "id": "ANoe94cuda1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Affinn Sentiment\n",
        "from afinn import Afinn\n",
        "\n",
        "afinn = Afinn()\n",
        "\n",
        "def get_afinn_sentiment(review):\n",
        "    return afinn.score(' '.join(review))\n",
        "\n",
        "merge_data[\"AFINN_Sentiment\"] = merge_data[\"Review\"].apply(get_afinn_sentiment)"
      ],
      "metadata": {
        "id": "37nroDa2duK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Bing_Liu_Sentiment Lexicon\n",
        "from nltk.corpus import opinion_lexicon\n",
        "\n",
        "nltk.download('opinion_lexicon')\n",
        "\n",
        "def get_bing_liu_sentiment(review):\n",
        "    positive_words = set(opinion_lexicon.positive())\n",
        "    negative_words = set(opinion_lexicon.negative())\n",
        "    sentiment_score = sum(1 for word in review if word in positive_words) - sum(1 for word in review if word in negative_words)\n",
        "    return sentiment_score\n",
        "\n",
        "merge_data[\"Bing_Liu_Sentiment\"] = merge_data[\"Review\"].apply(get_bing_liu_sentiment)"
      ],
      "metadata": {
        "id": "MQCk4wLaduiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing All Sentiment Lexicon Methods\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(merge_data)), merge_data['Vader_Sentiment'], color='blue', label='VADER')\n",
        "plt.scatter(range(len(merge_data)), merge_data['SentiWordNet_Sentiment'], color='green', label='SentiWordNet')\n",
        "plt.scatter(range(len(merge_data)), merge_data['AFINN_Sentiment'], color='orange', label='AFINN-111')\n",
        "plt.scatter(range(len(merge_data)), merge_data['Bing_Liu_Sentiment'], color='red', label=\"Bing Liu's Opinion Lexicon\")\n",
        "\n",
        "plt.axhline(y=0, color='gray', linestyle='--')  # Add a horizontal line at sentiment score = 0 (neutral)\n",
        "\n",
        "plt.xlabel('Review Index')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.title('Sentiment Scores by Different Lexicons')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5P6dDwZSd1IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the application of a scatter plot visualization, I have effectively analyzed and compared the performance of different sentiment lexicons. This technique allows me to discern which lexicon demonstrates superior efficacy in capturing and interpreting sentiments from the data.\n",
        "\n",
        "Upon meticulous examination of the scatter plot, it becomes evident that the Affin-111 and Bing_Liu_Sentiment lexicons exhibit notably superior accuracy and performance in capturing sentiments from the data. These lexicons prove to be more reliable and effective in interpreting the emotional content present in the analyzed text."
      ],
      "metadata": {
        "id": "YdAOvpffeucC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creaing a new DataFrame for Sentiment Analysis\n",
        "sentimental_df=merge_data[[\"Restaurant\", \"Review\", \"Vader_Sentiment\", \"SentiWordNet_Sentiment\", \"AFINN_Sentiment\", \"Bing_Liu_Sentiment\"]]"
      ],
      "metadata": {
        "id": "-o7YnEdSfbyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to map sentiment scores to labels\n",
        "def sentiment_label(score):\n",
        "    if score > 0:\n",
        "        return \"positive\"\n",
        "    elif score < 0:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\""
      ],
      "metadata": {
        "id": "Tn4vJ6ZAfhlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new column \"Overall_Sentiment\" based on the sentiment scores\n",
        "sentimental_df[\"Overall_Sentiment\"] =sentimental_df.mean(axis=1).apply(sentiment_label)"
      ],
      "metadata": {
        "id": "LGXWNmS2fj3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = sentimental_df[\"Overall_Sentiment\"].value_counts()\n",
        "\n",
        "# Create a pie chart to visualize the sentiment distribution\n",
        "plt.figure(figsize=(15, 5))\n",
        "colors = ['#4F6272', '#B7C3F3', '#DD7596']\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title(\"Overall Sentiment Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "egjXpDv_flo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clustering"
      ],
      "metadata": {
        "id": "OcSwCPwIfq65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Textual Data Preprocessing"
      ],
      "metadata": {
        "id": "W9OBWNIMf1F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "restrodata.isnull().sum()"
      ],
      "metadata": {
        "id": "vCJTqgTyf0ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new dataset for clustering\n",
        "cluster_df=restrodata[[\"Cost\", \"Cuisines\"]]"
      ],
      "metadata": {
        "id": "sbrPdM1Yf4Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_data=cluster_df.copy()\n",
        "cluster_df.head()"
      ],
      "metadata": {
        "id": "BPhFnSMkf5s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 1. Expand Contraction\n",
        "cluster_df[\"Cuisines\"] = cluster_df[\"Cuisines\"].apply(expand_contractions)"
      ],
      "metadata": {
        "id": "ZmSc4-fsf8G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Lower Casing\n",
        "cluster_df[\"Cuisines\"] = cluster_df[\"Cuisines\"].apply(to_lower)"
      ],
      "metadata": {
        "id": "jG7PKIrOgC7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Removing spaces which are separated by commas\n",
        "\n",
        "def remove_spaces_between_names(text):\n",
        "    # Split the text by commas\n",
        "    names = text.split(',')\n",
        "\n",
        "    # Remove spaces between individual names\n",
        "    cleaned_names = [name.strip().replace(' ', '') for name in names]\n",
        "\n",
        "    # Join the cleaned names with commas\n",
        "    cleaned_text = ', '.join(cleaned_names)\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "_ZD_RWFQgEyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df[\"Cuisines\"] = cluster_df[\"Cuisines\"].apply(remove_spaces_between_names)"
      ],
      "metadata": {
        "id": "QPYM3BpKgGvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 3. Removing Punctuations\n",
        "cluster_df[\"Cuisines\"] = cluster_df[\"Cuisines\"].apply(remove_punc)"
      ],
      "metadata": {
        "id": "18iSgEOPgIUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits.\n",
        "cluster_df[\"Cuisines\"] = cluster_df[\"Cuisines\"].apply(remove_urls)"
      ],
      "metadata": {
        "id": "-OFhtGqEgKFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 5. Removing Stopwords\n",
        "cluster_df[\"Cuisines\"] = cluster_df[\"Cuisines\"].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "3gnu_0zcgL4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "cluster_df[\"Cuisines\"] = cluster_df[\"Cuisines\"].apply(word_token)"
      ],
      "metadata": {
        "id": "WUq5F1d0gNtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.Cuisines.head()"
      ],
      "metadata": {
        "id": "LyGCTC04gPhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df['Cuisines'] = cluster_df['Cuisines'].apply(lambda cuisines: ' '.join(cuisines))"
      ],
      "metadata": {
        "id": "LnVyNakrgR-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "cluster_dff = vectorizer.fit_transform(cluster_df[\"Cuisines\"])\n",
        "cluster_dff"
      ],
      "metadata": {
        "id": "OEDX4BIYgTXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = pd.concat([cluster_df['Cost'], pd.DataFrame(cluster_dff.toarray(), columns=vectorizer.get_feature_names_out())], axis=1)\n",
        "features.head()"
      ],
      "metadata": {
        "id": "euSdSbY5gWvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvwF2osagZhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X = features[[\"Cost\"]].values\n",
        "features[\"Cost\"] = scaler.fit_transform(X)\n",
        "features.head()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###K_Means Algorithm\n"
      ],
      "metadata": {
        "id": "q1FR_rmVg93p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 K-means Implementation\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Number of clusters you want to create\n",
        "n_clusters = 3\n",
        "\n",
        "# Create an instance of the KMeans clustering algorithm\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "kmeans.fit(features)\n",
        "\n",
        "# Get the cluster labels for each data point\n",
        "features[\"Kmean_ClusterLabel\"] = kmeans.labels_\n"
      ],
      "metadata": {
        "id": "oAZnE3nNhFyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.head()"
      ],
      "metadata": {
        "id": "JEIH_lrvhHE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "6P85cN5VhYOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1\n",
        "## K-Means Algorithm\n",
        "### Check the optimum value of Cluster Using Elbow Method\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "inertias = []\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(features)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow curve\n",
        "plt.plot(range(1, 11), inertias, marker='o')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cUrfg7cjhI9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the elbow point is 3.So, we have choose correct value of n_cluster."
      ],
      "metadata": {
        "id": "WgKX6epthiCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the Cluster using PCA and t-sne\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "Features1 = features.drop(columns=[\"Kmean_ClusterLabel\"])\n",
        "cluster_labels = features[\"Kmean_ClusterLabel\"]\n",
        "\n",
        "#Reduce the dimensionality using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(Features1)\n",
        "\n",
        "# Reduce the dimensionality using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_result = tsne.fit_transform(Features1)\n",
        "\n",
        "# Create subplots to visualize PCA and t-SNE results side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot PCA\n",
        "axes[0].scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap=\"rainbow\")\n",
        "axes[0].set_title(\"PCA Visualization\")\n",
        "axes[0].set_xlabel(\"Principal Component 1\")\n",
        "axes[0].set_ylabel(\"Principal Component 2\")\n",
        "\n",
        "# Plot t-SNE\n",
        "axes[1].scatter(tsne_result[:, 0], tsne_result[:, 1], c=cluster_labels, cmap=\"rainbow\")\n",
        "axes[1].set_title(\"t-SNE Visualization\")\n",
        "axes[1].set_xlabel(\"t-SNE Component 1\")\n",
        "axes[1].set_ylabel(\"t-SNE Component 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LcHK3ZFkhim-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###AgglomerativeClustering"
      ],
      "metadata": {
        "id": "oaLzssubiGLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "metadata": {
        "id": "kBwIghA0iL6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining Cosine-Similarity\n",
        "cosine_sim = cosine_similarity(features.iloc[:,:-1])"
      ],
      "metadata": {
        "id": "s5h1YwIGiMgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the Agglomertive Algorithm\n",
        "n_clusters = 3\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='complete')\n",
        "agg_clustering.fit(1 - cosine_sim)"
      ],
      "metadata": {
        "id": "TFzSq6P_iOGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Label in dataset\n",
        "features[\"Agg_Cluster_Label\"] = agg_clustering.labels_\n",
        "features.head()"
      ],
      "metadata": {
        "id": "jklrC8vWiPsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Cluster for Agglomerative Clustering Algorithm\n",
        "\n",
        "Features2 = features.drop(columns=[\"Agg_Cluster_Label\", \"Kmean_ClusterLabel\"])\n",
        "cluster_labels = features[\"Agg_Cluster_Label\"]\n",
        "\n",
        "# Reduce the dimensionality using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(Features2)\n",
        "\n",
        "# Reduce the dimensionality using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_result = tsne.fit_transform(Features2)\n",
        "\n",
        "# Visualizing PCA and t-SNE results side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot PCA\n",
        "axes[0].scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap=\"rainbow\")\n",
        "axes[0].set_title(\"PCA Visualization\")\n",
        "axes[0].set_xlabel(\"Principal Component 1\")\n",
        "axes[0].set_ylabel(\"Principal Component 2\")\n",
        "\n",
        "# Plot t-SNE\n",
        "axes[1].scatter(tsne_result[:, 0], tsne_result[:, 1], c=cluster_labels, cmap=\"rainbow\")\n",
        "axes[1].set_title(\"t-SNE Visualization\")\n",
        "axes[1].set_xlabel(\"t-SNE Component 1\")\n",
        "axes[1].set_ylabel(\"t-SNE Component 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9cpqeOrsiU4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluters\n",
        "\n",
        "# Group the data by the cluster labels\n",
        "cluster_groups = features.groupby('Kmean_ClusterLabel')\n",
        "\n",
        "# Iterate through each cluster and analyze the characteristics\n",
        "for cluster_label, cluster_data in cluster_groups:\n",
        "    print(f\"Cluster {cluster_label}:\")\n",
        "\n",
        "    # Drop the unwanted columns before analyzing the cuisines\n",
        "    cluster_data = cluster_data.drop(['Kmean_ClusterLabel', 'Agg_Cluster_Label'], axis=1)\n",
        "\n",
        "    # Calculate the most frequent cuisines in the cluster\n",
        "    most_frequent_cuisines = cluster_data.drop('Cost', axis=1).sum().nlargest(5)\n",
        "    print(\"Most frequent cuisines:\")\n",
        "    print(most_frequent_cuisines)\n",
        "\n",
        "    # Calculate the cost range in the cluster\n",
        "    cost_range = (cluster_data['Cost'].min(), cluster_data['Cost'].max())\n",
        "    print(f\"Cost range: {cost_range[0]} - {cost_range[1]}\")\n",
        "\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "id": "9mrh3rYaiXA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulaizing all three cluster by wrodcloud\n",
        "\n",
        "# Group the data by the cluster labels\n",
        "cluster_groups = features.groupby('Kmean_ClusterLabel')\n",
        "\n",
        "# Iterate through each cluster and create a word cloud for the most frequent cuisines\n",
        "for cluster_label, cluster_data in cluster_groups:\n",
        "    # Drop the unwanted columns before analyzing the cuisines\n",
        "    cluster_data = cluster_data.drop(['Kmean_ClusterLabel', 'Agg_Cluster_Label'], axis=1)\n",
        "\n",
        "    # Calculate the most frequent cuisines in the cluster\n",
        "    most_frequent_cuisines = cluster_data.drop('Cost', axis=1).sum().nlargest(10)\n",
        "\n",
        "    # Convert the most frequent cuisines into a dictionary format (word: frequency)\n",
        "    cuisines_dict = most_frequent_cuisines.to_dict()\n",
        "\n",
        "    # Create a word cloud for the cluster\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(cuisines_dict)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f\"Cluster {cluster_label} - Most Frequent Cuisines\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Lm_arpU9iac0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully accomplished the objectives of clustering restaurants based on their features and conducting sentiment analysis on user reviews. Through clustering, we gained valuable insights into the grouping of restaurants, helping both users and businesses make informed decisions. The sentiment analysis allowed us to understand the sentiments expressed by users in their reviews, providing businesses with valuable feedback to enhance their services and improve the overall user experience.\n",
        "\n",
        "The utilization of various data preprocessing techniques, such as text vectorization and feature normalization, played a crucial role in preparing the data for clustering and sentiment analysis. We employed popular machine learning algorithms, including K-Means and Agglomerative Clustering, to create meaningful clusters of restaurants based on their similarities.\n",
        "\n",
        "For future enhancements, more advanced clustering algorithms and sentiment analysis techniques could be explored to further refine the results. Additionally, incorporating additional features such as images and menus of the restaurants might provide more comprehensive insights.\n",
        "\n",
        "Overall, this project demonstrates the potential of leveraging data analytics to gain valuable insights into the restaurant industry, aiding both users in making informed choices and businesses in enhancing their services to meet customer expectations."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}